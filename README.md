# 2021机器学习课程项目-基于深度学习的中文手写体识别

## 项目架构

```txt
├───.idea          #idea工程配置文件夹
│   └───inspectionProfiles
├───cannopy        #忽略 其他项目文件临时存放
├───checkpoints    #权重 保存点 存储文件夹
├───dataset        #数据集处理相关
│   ├───HWDB       #中科院HWDB手写数据集处理
│   ├───HW_SELF    #手写中文处理
│   ├───MNIST      #MNIST数字数据集
│   └───TFrecord   #制作TFrecord转储文件
├───fisher         #fisher代码
└───train_DL       #训练代码
    ├───assets     #验证集存储位置
    └───my_logs    #训练日志存储位置
```

## 数据集

为了保证在神经网络训练的过程中，有足够多的样本图片进行训练操作，本项目采用了两个互联网公开数据集。

分别为**CASIA-HWDB**与**MNIST**数据集，两个数据集均进行了图片预处理与图片增强，并且统一将所有图片resize为(64，64)的图片大小。

### MNIST数据集

MNIST 数据集来自美国国家标准与技术研究所, 数据集由来自 250 个不同人手写的数字构成, 其中 50% 是高中学生, 50% 来自人口普查局的工作人员。其中已经预先按照8:2的比例分配好了训练集与验证集，每张图片都是28\*28的分辨率，为了配合HWDB数据集，在之后的操作中，会手动将所有图片放缩为64\*64的图片大小。

### CASIA-HWDB数据集

CASIA-HWDB数据集为中科院自动化研究所创建，主要收集了成年人手写简体中文汉字的图片，并对每个单独中文简体汉字进行切割。每个图片在数据集里由多个属性构成，分别为图片大小、字符标识号、图片宽度、图片高度、Bitmap格式的图片五个部分组成，所以为了增强图片并重新与HWDB数据集混合，需要将所有图片进行解压操作。该数据集格式如图1。与MNIST数据集类似，HWDB数据集同样预先分好了训练集与验证集，因此不需要再此分配。

![图1 GNT文件格式内部属性](.\md_picture\1.png)

## 数据集准备工作

**请注意：**考虑到正则表达式的复杂性，在该工程中的所有路径都是绝对路径的处理，如需复现需要手动处理相关路径问题

1. 首先处理MNIST数据集 ，从该网址[MNIST数据集](http://yann.lecun.com/exdb/mnist/)下载四个文件到本地目录

2. 修改/dataset/MNIST/process.py 中5-6行解压目标目录 8-11行数据集本地存在的路径后运行该文件

3. 修改/dataset/MNIST/resize.py 中5-6行训练集或验证集存在路径 并运行进行resize操作

4. 处理MNIST数据集较为简单，仅需要做一次resize操作，在完成HWDB数据集工作后，将训练集与验证集分别复制到对应的目录即可

5. 接着处理HWDB数据集，从该网址[HWDB数据集](http://www.nlpr.ia.ac.cn/databases/handwriting/Home.html)下载HWDB1.1两个文件到本地目录

6. 修改/dataset/HWDB/process.py 中6行保存目录以及57 65行输出文件目录并运行，获得解压后的所有图片，并且每个单字的图片存入对应的文件夹，如图2。

   ![图2 解压后的数据集示例](./md_picture/2.png)

   **注意：**在后续图片处理过程中，发现“丙”字内有一张图片错误，尚不清楚造成的原因，在本项目中直接将此图删除，错误图片如图3。

   ![图3 错误图片](./md_picture/3.png)

7. 将MNIST数据集复制到本文件夹，并将0-9分别对应重命名为train(test)03755-train(test)03765

   **注意：**不同版本的pickle序列化工具会导致问题，并且为了后续的简便操作，这里是直接使用了其他项目给出的词汇表，一行对应一个字，我们要做的在该文件后加上0-9十行，该文件为/dataset/characters.txt。

8. 修改/dataset/HWDB/pictureEnhance.py 中 10行数据集存放路径 29-30行获取标签 39行新的存储路径并运行 对所有图片进行统一的数据处理及增强

   1. 首先进行一次resize为64\*64的操作并存储

   2. 在此基础上添加高斯噪声并存储

   3. 在此基础上使用锐化操作并存储

   4. 完成以上工作后能看到如图5的形式，所有图片都是64\*64的像素，每张图片都生成了四张新的图片

      ![图5 生成的数据集](./md_picture/4.png)

9. 因为生成的数据集是完全没有被打乱的，每个连续的类大约有1000张图片，生成TFrecord文件后无法完全打乱。因此需要手动locally shuffle，该步骤的原理将在后续章节后给出原因。修改/dataset/HWDB/shuffle.py 中5 17行数据集存储路径，6行目标路径并允许进行手动打乱。

10. 在完成以上步骤后，数据集如图6所示，已经被完全打乱。

    ![图6 打乱后的数据集](./md_picture/5.png)

    因为涉及到大量的IO，代码运行时间依系统CPU与磁盘能力决定。

    **注意：**请提前将整个数据集目录从系统索引/Everything索引中移除，否则会导致更大的系统负荷

## 神经网络架构

